{
  "id": "replication-sharding",
  "title": "Database Replication and Sharding Patterns",
  "category": "databases",
  "content": "Replication and sharding are two fundamental techniques for scaling databases horizontally. While they serve different purposes, they're often used together to build highly scalable and available database systems.\n\nDatabase Replication:\n\nReplication involves maintaining multiple copies of data across different database servers. This provides redundancy, improves read performance, and increases availability.\n\nReplication Types:\n\n1. Master-Slave (Primary-Replica):\n   - One primary node handles all writes\n   - Multiple replica nodes handle reads\n   - Changes replicate from primary to replicas\n   - Asynchronous or synchronous replication\n   - Pros: Simple, scalable reads, fault tolerance\n   - Cons: Write bottleneck, replication lag\n\n2. Master-Master (Multi-Primary):\n   - Multiple nodes accept writes\n   - Bidirectional replication\n   - Conflict resolution required\n   - Higher availability\n   - Pros: No write bottleneck, high availability\n   - Cons: Complex conflict resolution, eventual consistency\n\n3. Chain Replication:\n   - Linear chain of replicas\n   - Writes to head, reads from tail\n   - Strong consistency guarantees\n   - Pros: Consistent reads, fault tolerance\n   - Cons: Higher latency, complex recovery\n\nReplication Strategies:\n\nSynchronous Replication:\n- Primary waits for replica acknowledgment\n- Strong consistency guaranteed\n- Higher latency on writes\n- Use when: Data consistency is critical\n- Examples: Financial transactions, inventory systems\n\nAsynchronous Replication:\n- Primary doesn't wait for replicas\n- Lower write latency\n- Eventual consistency\n- Replication lag possible\n- Use when: Performance > immediate consistency\n- Examples: Social media feeds, analytics\n\nSemi-Synchronous:\n- Wait for at least one replica\n- Balance between consistency and performance\n- Reduces data loss risk\n- Common in production systems\n\nReplication Lag:\n\nCauses:\n- Network latency between data centers\n- High write volume overwhelming replicas\n- Slow queries blocking replication\n- Hardware resource constraints\n\nMitigation:\n- Monitor lag metrics continuously\n- Use read-your-writes consistency\n- Direct critical reads to primary\n- Scale replica hardware\n- Optimize replication queries\n\nDatabase Sharding:\n\nSharding (horizontal partitioning) splits data across multiple database instances based on a shard key. Each shard contains a subset of the total data.\n\nWhy Shard:\n- Single database hitting resource limits\n- Data too large for one server\n- Write throughput exceeds single server capacity\n- Need geographic data distribution\n- Cost optimization (scale out vs up)\n\nSharding Strategies:\n\n1. Range-Based Sharding:\n   - Divide data by ranges of shard key\n   - Example: User IDs 1-10000 → Shard 1, 10001-20000 → Shard 2\n   - Pros: Simple, ordered data access\n   - Cons: Hotspots if data not evenly distributed\n   - Use case: Time-series data, sequential IDs\n\n2. Hash-Based Sharding:\n   - Use hash function on shard key\n   - Example: hash(user_id) % num_shards\n   - Pros: Uniform distribution, no hotspots\n   - Cons: Range queries difficult, resharding complex\n   - Use case: User data, session data\n\n3. Geographic Sharding:\n   - Shard by location/region\n   - Example: US users → US shard, EU users → EU shard\n   - Pros: Data locality, regulatory compliance\n   - Cons: Uneven distribution, cross-region queries\n   - Use case: Multi-region apps, GDPR compliance\n\n4. Directory-Based Sharding:\n   - Lookup table maps keys to shards\n   - Example: Shard mapping service\n   - Pros: Flexible, easy to rebalance\n   - Cons: Lookup overhead, single point of failure\n   - Use case: Complex sharding logic\n\n5. Entity-Based Sharding:\n   - Shard by entity type or tenant\n   - Example: Each company gets its own shard\n   - Pros: Isolation, simple queries\n   - Cons: Uneven sizes, tenant hotspots\n   - Use case: Multi-tenant SaaS applications\n\nSharding Challenges:\n\n1. Cross-Shard Queries:\n   - Queries spanning multiple shards are slow\n   - Need scatter-gather pattern\n   - Solution: Denormalize data, cache results\n\n2. Cross-Shard Joins:\n   - Joins across shards extremely expensive\n   - Solution: Avoid with denormalization, application-level joins\n\n3. Transactions:\n   - ACID transactions across shards are difficult\n   - Distributed transactions are slow\n   - Solution: Design for single-shard transactions, use sagas\n\n4. Resharding:\n   - Adding/removing shards requires data movement\n   - Can cause downtime\n   - Solution: Use consistent hashing, plan for resharding\n\n5. Hotspots:\n   - Popular data creates hot shards\n   - Uneven distribution causes performance issues\n   - Solution: Choose good shard key, split hot shards\n\nChoosing a Shard Key:\n\nCriteria:\n- High cardinality (many unique values)\n- Even distribution of data\n- Queries mostly use shard key\n- Rarely needs updating\n- Business-aligned (user_id, tenant_id)\n\nBad Shard Keys:\n- Timestamp (creates hot spots)\n- Boolean values (only 2 shards)\n- Auto-increment ID without hashing\n- Values that change frequently\n\nGood Shard Keys:\n- User ID (hash-based)\n- Tenant ID (multi-tenant apps)\n- Geographic region + user ID\n- Composite keys (multiple fields)\n\nCombining Replication and Sharding:\n\nCommon Pattern:\n- Shard data horizontally\n- Replicate each shard\n- Read from replicas, write to primary\n- Benefits: Scalability + availability\n\nArchitecture:\n```\nShard 1 Primary ──→ Shard 1 Replica 1\n                └──→ Shard 1 Replica 2\n\nShard 2 Primary ──→ Shard 2 Replica 1\n                └──→ Shard 2 Replica 2\n```\n\nConsistency Models:\n\nStrong Consistency:\n- All reads see latest write\n- Synchronous replication required\n- Higher latency\n- Use: Financial data, inventory\n\nEventual Consistency:\n- Replicas eventually converge\n- Asynchronous replication\n- Lower latency, higher availability\n- Use: Social feeds, recommendations\n\nCausal Consistency:\n- Preserves cause-effect relationships\n- Balance between strong and eventual\n- Use: Messaging, collaborative editing\n\nMonitoring Sharded Systems:\n\nKey Metrics:\n- Shard size distribution\n- Query latency per shard\n- Replication lag\n- Cross-shard query frequency\n- Hot shard identification\n- Write throughput per shard\n\nTools:\n- Vitess: Sharding solution for MySQL\n- Citus: Distributed PostgreSQL\n- MongoDB: Built-in sharding\n- Cassandra: Automatic sharding\n- CockroachDB: Transparent sharding\n\nMigration Strategies:\n\nFrom Single DB to Sharded:\n1. Add sharding layer (no data movement)\n2. Implement application-level routing\n3. Gradually move data to shards\n4. Monitor and optimize\n5. Decommission original DB\n\nResharding:\n1. Double write to old and new shards\n2. Backfill data to new shards\n3. Switch reads to new shards\n4. Stop writing to old shards\n5. Verify and cleanup\n\nBest Practices:\n\n1. Start Simple:\n   - Begin with replication only\n   - Shard when necessary, not preemptively\n   - Plan for sharding from day one\n\n2. Choose Shard Key Carefully:\n   - Analyze query patterns\n   - Test distribution\n   - Hard to change later\n\n3. Monitor Continuously:\n   - Track shard metrics\n   - Identify hotspots early\n   - Plan capacity growth\n\n4. Automate Operations:\n   - Automated failover\n   - Health checks\n   - Backup and recovery\n\n5. Design for Failure:\n   - Assume shards will fail\n   - Implement retry logic\n   - Use circuit breakers\n\nAnti-Patterns:\n\n- Premature sharding (optimize first)\n- Poor shard key selection\n- Cross-shard transactions everywhere\n- No monitoring of shard health\n- Manual failover processes\n- Ignoring replication lag",
  "source": "Database Scaling Architecture Guide",
  "url": "https://example.com/replication-sharding",
  "tags": [
    "databases",
    "sharding",
    "replication",
    "distributed-systems",
    "scaling"
  ],
  "difficulty": "advanced",
  "timestamp": "2024-11-20T12:00:00.000Z"
}